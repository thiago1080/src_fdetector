{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "sys.path.append(join(Path(os.getcwd()).parent, 'lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.1.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import collections\n",
    "from importlib import reload\n",
    "from  mainlib import fasorial, lista, sdict, mm, mm_sep\n",
    "from unidecode import unidecode\n",
    "from of_proc import *\n",
    "from os.path import join\n",
    "from os import listdir as ld\n",
    "from unidecode import unidecode\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_info(path, tem_perfil_funcionamento):\n",
    "    global PATH, OUTPUT_PATH, PROCESSED_PATH, PERFIS_PATH, HORARIOS_PATH, PATH_MM, PATH_FAS, \\\n",
    "    DEG, MOR, PER, FAS, OUTPUT_HEADER,FAS_CONCAT, MM_CONCAT, PICKLES_CONCAT, OUT_INDEX, COLS_LISTA, \\\n",
    "    OUT, DATA_DIR, LISTA_CAD_PATH, PROCESSADOS_PICKLE, ERROS_PICKLE, LOGS_DIR, ADJACENCIAS_PATH\n",
    "    with open(path) as f:\n",
    "        info = json.load(f)\n",
    "        PATH_FAS = info['paths']['fasoriais2']['input']\n",
    "        PERFIS_PATH = info['paths']['perfis']['perfis']\n",
    "        HORARIOS_PATH = info['paths']['perfis']['horarios']\n",
    "        PATH_MM = info['paths']['mm']['input']\n",
    "        DEG = info['paths']['nn_models']['degrau']\n",
    "        MOR = info['paths']['nn_models']['morro']\n",
    "        PER = info['paths']['nn_models']['perfil']\n",
    "        FAS = info['paths']['nn_models']['fasorial']\n",
    "        COLS_LISTA = info['lista2']['columns']  \n",
    "        OUTPUT_HEADER = info['output']['header']\n",
    "        OUT_INDEX = info['output']['header']\n",
    "        OUT = info['paths']['output']\n",
    "        DATA_DIR = info['paths']['base_dir']\n",
    "        LISTA_CAD_PATH = info['paths']['lista-set-1']\n",
    "        PROCESSADOS_PICKLE = info['paths']['processados']\n",
    "        ERROS_PICKLE = info['paths']['erros']\n",
    "        LOGS_DIR = info['paths']['logs']\n",
    "        if tem_perfil_funcionamento:\n",
    "            ADJACENCIAS_PATH = info['paths']['adjacencias']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = '../../info/info-LIGHT.json'\n",
    "gd = False\n",
    "tem_perfil_funcionamento = True\n",
    "\n",
    "# Se for remover registros de tensões zeradas, configurar se preenche ou não os registros removidos\n",
    "# Pra processamento EDP-ES = False. Para a Light = True\n",
    "fill_rows = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dk(d,k=0): return list(d.values())[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_info(json_file, tem_perfil_funcionamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_horario_info2(json_file):\n",
    "    global ATIV_PATH\n",
    "    with open(json_file) as jinfo:\n",
    "        info = json.load(jinfo)\n",
    "        HORARIOS_PATH = info['paths']['perfis']['horarios']\n",
    "        PERFIS_PATH = info['paths']['perfis']['perfis']\n",
    "        ATIV_PATH = info['paths']['perfis']['mapa']\n",
    "\n",
    "\n",
    "    perfis = pd.read_csv(PERFIS_PATH, sep = ';')\n",
    "    ativs = pd.read_csv(ATIV_PATH, sep = ';')\n",
    "\n",
    "    #perfis.columns = ['id', 'instalacao', 'cpf_cnpj', 'razao social', 'endereco_cadastro', 'ramo_de_atividade','None1', 'None2']\n",
    "    \n",
    "    dates = re.compile(\"[0-9]{2}:[0-9]{2}\")\n",
    "    dateparser = lambda x: pd.datetime.strptime(x, '%H:%M') if type(x) == str  and dates.match(x) else pd.NaT\n",
    "    cols = ['name','inicio_dia_util','fim_dia_util', 'inicio_sabado', 'fim_sabado', 'inicio_domingo', 'fim_domingo', 'inicio_feriado', 'fim_feriado']\n",
    "    horarios = pd.read_csv(HORARIOS_PATH, sep = ';',skipinitialspace=True, skiprows = 2, na_values = np.NaN, names = cols,\n",
    "                          date_parser = dateparser, parse_dates = [1,2,3,4,5,6,7,8])\n",
    "    \n",
    "    \n",
    "    horarios.set_index('name', inplace = True)\n",
    "    horarios.dropna(inplace=True)\n",
    "\n",
    "    perfis.set_index('instalacao', inplace=True)\n",
    "    perfis = (perfis.groupby(level=0).first())\n",
    "\n",
    "    \n",
    "    return horarios, perfis, ativs\n",
    "\n",
    "def get_files_4(nomes, dim,  CAMINHO):\n",
    "    not_found = []\n",
    "    dif = {}\n",
    "    for i in nomes:\n",
    "        if i in dim:\n",
    "            tmp_list = []\n",
    "            search = ab.search_list_2(str(i),os.listdir(CAMINHO))\n",
    "            if search:\n",
    "                dif[i] = search\n",
    "            else:\n",
    "                not_found.append(i)                \n",
    "    return dif, not_found\n",
    "\n",
    "#Criar dicionario de categorias\n",
    "def insta2horario(HORARIOS_PATH):\n",
    "    horarios = pd.read_csv(HORARIOS_PATH, sep=',')\n",
    "    horarios.drop(columns=horarios.columns[0],inplace=True)\n",
    "    horarios.columns = ['nome', 'cat', 'dias_uteis_i',\n",
    "           'dias_uteis_f', 'sabado_i', 'sabado_f', 'domingo_i',\n",
    "           'domingo_f', 'feriado_i', 'feriado_f']\n",
    "    horarios['nome'] = horarios['nome'].apply(unidecode)\n",
    "    dfcat = horarios.iloc[:,:2]\n",
    "    dfcat.set_index('nome',inplace=True)\n",
    "    scat = dfcat['cat']\n",
    "    dcat = scat.to_dict()\n",
    "    horarios.set_index('nome', inplace=True)\n",
    "    func = {k: [pd.Timestamp(v) for v in horarios.loc[k,'dias_uteis_i':'feriado_f']] for k in horarios.index}\n",
    "    return func\n",
    "\n",
    "def per2cluster(func, ATIV):\n",
    "    dfativ = pd.read_csv(ATIV, sep=';', names = ['ramo', 'perfil'])\n",
    "    dfativ=dfativ.applymap(unidecode)\n",
    "    dfativ.set_index('ramo', inplace=True)\n",
    "    a2p = dfativ.loc[:,'perfil'].T.to_dict()\n",
    "    a2p.pop('Ramo de Atividade')\n",
    "    p2a = dict((v,k) for k,v in a2p.items())\n",
    "    perfunc=PERFIS['ramo_de_atividade'].map(a2p).map(func)\n",
    "    return perfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global df_mm_new\n",
    "mmm = mm(json_file)\n",
    "df_mm_new = pd.read_csv(PATH_MM,date_parser = mmm.dateparser, index_col=[0], sep=';')\n",
    "dummy_mm = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar modelos neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleções principais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global LISTA_CAD\n",
    "global PERFIL, DEGRAU, MORRO, NN_FAS\n",
    "global files_fas, files_mm\n",
    "global HORARIOS, PERFIS, ATIVS, DPERFIS, DPERFIS_NORMAL, DPERFIS_TROCA\n",
    "global dim, dmi, df_fas, dfiles_mm, dfiles_fas_troca, dfiles_mm_troca, dfiles_fas_normal, dfiles_mm_normal\n",
    "global o_i_normal, o_i_troca, o_instas, list_i_normal, list_i_troca, list_intas, gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "PERFIL, DEGRAU, MORRO, NN_FAS = load_models(PER, DEG, MOR, FAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/data/maio_2019/setembro/lista-cad-pnt-20190820.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LISTA_CAD_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    LISTAF = pd.read_csv(LISTA_CAD_PATH, names = COLS_LISTA, header=0, sep=';',engine='python')\n",
    "except:\n",
    "    LISTAF = pd.read_csv(LISTA_CAD_PATH, names = COLS_LISTA, header=0, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTAF['instalacao'] = pd.DataFrame(LISTAF['instalacao'].apply(lambda x: '{0:0>10}'.format(x)))\n",
    "try:\n",
    "    LISTAF['medidor'] = pd.DataFrame(LISTAF['medidor'].astype(int).apply(lambda x: '{0:0>8}'.format(x)))\n",
    "except:    \n",
    "    LISTAF['medidor'] = pd.DataFrame(LISTAF['medidor'].apply(lambda x: '{0:0>8}'.format(x)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#EDP-ES\n",
    "faltantes = ['0000120537']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# EDP-ES\n",
    "faltantes = [\n",
    "'0000081002',\n",
    "'0000357754',\n",
    "'0000586609',\n",
    "'0001659074',\n",
    "'0009502437',\n",
    "'0160214293',\n",
    "'0160557791',\n",
    "'0160581342',\n",
    "'0160669410'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHT\n",
    "faltantes = [\n",
    "'BT2A411444175'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTAF = LISTAF[LISTAF['instalacao'].isin(faltantes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mainlib.lista object at 0x7f5829595ba8>\n"
     ]
    }
   ],
   "source": [
    "LISTA_CAD = get_list(json_file)\n",
    "LISTA_CAD.df =  LISTAF\n",
    "if tem_perfil_funcionamento:\n",
    "    HORARIOS, PERFIS, ATIVS = get_horario_info2(json_file)\n",
    "dim, dmi = dicts(LISTA_CAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estruturas auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns_remove = ['cliente', 'medidor', 'data_inicio', 'data_fim', 'lote', 'ssn', 'troca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(LISTA_CAD.df.columns[~LISTA_CAD.df.columns.isin(list_columns_remove)==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(cols[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LISTA_CAD.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_without_instal = cols.copy()\n",
    "del(cols_without_instal[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "troca = False\n",
    "o_i_normal = collections.OrderedDict(sorted(LISTA_CAD.mes_days(troca, cols).items()))\n",
    "troca = True\n",
    "o_i_troca = collections.OrderedDict(sorted(LISTA_CAD.mes_days(troca, cols).items()))\n",
    "o_instas =  collections.OrderedDict(sorted({**o_i_normal, **o_i_troca}.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'A_base' from '/home/ubuntu/data/code/ref_fdetector/src/lib/A_base.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import A_base\n",
    "reload(A_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import A_base as ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfiles_fas, nf_fas= get_files_4(list(o_instas.keys()), dim,  PATH_FAS)\n",
    "ddirs_fas = {k:join(PATH_FAS,v) for k,v in dfiles_fas.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista , perfis e horários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário de instalacao para horario de funcionamento\n",
    "if tem_perfil_funcionamento:\n",
    "    func =  insta2horario(HORARIOS_PATH)\n",
    "    # Dicionário de perfis para clusters\n",
    "    perfunc = per2cluster(func, ATIV_PATH)\n",
    "    percats = PERFIS['cat'].fillna(value=0).apply(int).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " l___________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pdump(o_instas, '../../pickles/o_instas.pkl')\n",
    "#pdump(func, '../../pickles/func.pkl')\n",
    "#pdump(perfunc, '../../pickles/perfunc.pkl')\n",
    "pdump(ddirs_fas, '../../pickles/ddirs_fas.pkl')\n",
    "pdump(dim, '../../pickles/dim.pkl')\n",
    "pdump(dmi, '../../pickles/dmi.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_hora = []\n",
    "err_msg = {}\n",
    "missing = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSADOS_PICKLE,'rb') as p:\n",
    "    processados=pickle.load(p)\n",
    "with open(ERROS_PICKLE,'rb') as p:\n",
    "    erros = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../logs/0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(LOGS_DIR,str(log_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(os.path.join(LOGS_DIR,str(log_count)), 'at+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(string):\n",
    "    print(string)\n",
    "    log_file.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dici2list(d_insta_to_days):\n",
    "    ddays={}\n",
    "    ddays[4],ddays[5],ddays[6],ddays[7] = d_insta_to_days[12], d_insta_to_days[1], d_insta_to_days[2], d_insta_to_days[3], d_insta_to_days[4]\n",
    "    return ddays\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn' . option None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PERFIS.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "padrao = [pd.Timestamp('2019-09-20 00:00:00'),\n",
    " pd.Timestamp('2019-09-20 23:59:00'),\n",
    " pd.Timestamp('2019-09-20 00:00:00'),\n",
    " pd.Timestamp('2019-09-20 23:59:00'),\n",
    " pd.Timestamp('2019-09-20 00:00:00'),\n",
    " pd.Timestamp('2019-09-20 23:59:00'),\n",
    " pd.Timestamp('2019-09-20 00:00:00'),\n",
    " pd.Timestamp('2019-09-20 23:59:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loopy2(d_insta_to_days, d_insta_to_files_fas, d_insta_to_files_mm,PATH_FAS, PATH_MM, output_dir,sepfas, sepmm, gd, tem_perfil_funcionamento, fill_rows):\n",
    "    df_out = pd.DataFrame(index=OUT_INDEX)\n",
    "    global difas, dimm\n",
    "    global dfas, dmm,  dimm\n",
    "    global threshold_tensoes_zeradas, df_tensoes_zeradas\n",
    "    difas, dimm = sdict(), sdict()\n",
    "    c = 0\n",
    "    c1 = 0\n",
    "    df_tensoes_zeradas = [pd.DataFrame(columns=['Mes', 'Instalacao'])]\n",
    "    threshold_tensoes_zeradas = 0.5\n",
    "    if tem_perfil_funcionamento:\n",
    "        with open(ADJACENCIAS_PATH,'rb') as p:\n",
    "            adjacencias_df = pickle.load(p)\n",
    "    else:\n",
    "        adjacencias_df = []\n",
    "    for instalacao in d_insta_to_days:\n",
    "        c1 = c1 + 1\n",
    "        inst_year = o_instas[instalacao][cols_without_instal[0]].year\n",
    "        if (instalacao  not in processados):\n",
    "            log(str(c)+'--'+'######## ' + instalacao + ' #########')\n",
    "            medidor = dim[instalacao][0] if type(dim[instalacao]) is list else dim[instalacao]\n",
    "            try:      \n",
    "                print('------ 0')\n",
    "                if tem_perfil_funcionamento:\n",
    "                    ramo = percats[instalacao] if instalacao in percats else 0\n",
    "                else:\n",
    "                    ramo = 0\n",
    "\n",
    "                if ramo == 0:\n",
    "                    profile_err = True\n",
    "                else:\n",
    "                    profile_err = False\n",
    "\n",
    "                print('------ 1')\n",
    "                if tem_perfil_funcionamento:\n",
    "                    hours = perfunc[instalacao] if instalacao in perfunc else padrao\n",
    "                else:    \n",
    "                    hours = padrao\n",
    "                print('------ 2')\n",
    "                ddays = d_insta_to_days[instalacao] if instalacao in d_insta_to_days else default_ddays_2()\n",
    "                print('------ 3')\n",
    "                dfas = fasorial(json_file, name = instalacao, ddays=ddays)     \n",
    "                print('------ 4')\n",
    "                dfas.outputs['perfil'] = ramo\n",
    "                print('------ 5')\n",
    "                dfas.read_fasorial(d_insta_to_files_fas[instalacao], dtype=None, index_col=[5], sep = sepfas)\n",
    "                print('------ 6')\n",
    "                dmm = mm(json_file, name = instalacao, ddays = ddays, ramo=ramo)\n",
    "                print('------ 7')\n",
    "                if dim[instalacao] in df_mm_new.columns:\n",
    "                    dmm.df = pd.DataFrame(df_mm_new[dim[instalacao]])\n",
    "                else:\n",
    "                    dmm.df = pd.DataFrame(df_mm_new[str(int(dim[instalacao]))])                 \n",
    "\n",
    "                print('------ 8')\n",
    "                proc_fas(dfas,ddays, NN_FAS, json_file, hours = dfas.hours, gd=gd, fill_rows=fill_rows, threshold_tensoes_zeradas = threshold_tensoes_zeradas, df_tensoes_zeradas = df_tensoes_zeradas)\n",
    "                processa_mm(dmm, ddays, PERFIL, DEGRAU, MORRO, json_file, adjacencias_df, tem_perfil_funcionamento, profile_err)\n",
    "                print(d_insta_to_files_fas[instalacao])\n",
    "                print('################################################')\n",
    "                #Cria um dicionário de outputs por mês para o fasorial\n",
    "                for y in dfas.months:\n",
    "                    dfas.outputs[y] = {}\n",
    "                    for month in dfas.months[y]:\n",
    "                        if len(df_tensoes_zeradas[0][(df_tensoes_zeradas[0]['Mes']==month) & (df_tensoes_zeradas[0]['Instalacao']==dfas.name)]) == 0:\n",
    "                            dfas.outputs[y][month] = dfas.months[y][month].outputs \n",
    "                df_out = cat_out2(df_out, dmm.outputs, dfas.outputs, ddays,OUT_INDEX, cols_without_instal, inst_year)\n",
    "                processados.append(instalacao)\n",
    "                difas[instalacao] = dfas\n",
    "                dimm[instalacao] = dmm                \n",
    "                c = c+1                \n",
    "            except Exception as e:\n",
    "                erros.append(instalacao)\n",
    "                err_msg[instalacao] = traceback.format_exc()\n",
    "                print('++++++Erro 2++++++++++ \\n'+ str(traceback.format_exc())+'\\n++++++++++++++++++++')\n",
    "    df_out = df_out.T\n",
    "    df_out.to_csv(output_dir + 'new.csv')\n",
    "    for index, row in df_tensoes_zeradas[0].iterrows():\n",
    "        index_to_drop = df_out[(df_out['Nome'] == row['Instalacao']) & (df_out['Mês'] == row['Mes'])].index\n",
    "        df_out.drop(index_to_drop , inplace=True)\n",
    "    df_tensoes_zeradas[0].to_csv(output_dir + 'tensoes_zeradas.csv') \n",
    "    df_out.to_csv(output_dir + 'newAfter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mainlib import mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0--######## BT2A411444175 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "------ 6\n",
      "------ 7\n",
      "------ 8\n",
      "{5: Timestamp('2019-05-13 00:00:00'), 6: Timestamp('2019-06-13 00:00:00'), 7: Timestamp('2019-07-11 00:00:00')}\n",
      "Registros com Tensão Zerada =  0\n",
      "Total de Registros =  8631\n",
      "Proporcao de Zerados =  0.0 %\n",
      "Possui Tensao Zerada =  False\n",
      "Possui Tensao Zerada =  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:766: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros com Tensão Zerada =  0\n",
      "Total de Registros =  8631\n",
      "Proporcao de Zerados =  0.0 %\n",
      "Possui Tensao Zerada =  False\n",
      "Possui Tensao Zerada =  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:766: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros com Tensão Zerada =  0\n",
      "Total de Registros =  8634\n",
      "Proporcao de Zerados =  0.0 %\n",
      "Possui Tensao Zerada =  False\n",
      "Possui Tensao Zerada =  False\n",
      "{5: Timestamp('2019-05-13 00:00:00'), 6: Timestamp('2019-06-13 00:00:00'), 7: Timestamp('2019-07-11 00:00:00')}\n",
      "{2019: {6: <mainlib.mm object at 0x7f58295183c8>, 7: <mainlib.mm object at 0x7f5800c2e2e8>}}\n",
      "Ano/Mês =  2019 / 5\n",
      "Perfil Cadastrado   =  1\n",
      "Perfil Classificado =  3\n",
      "Severidade =  1\n",
      "Ano/Mês =  2019 / 6\n",
      "Perfil Cadastrado   =  1\n",
      "Perfil Classificado =  3\n",
      "Severidade =  1\n",
      "Ano/Mês =  2019 / 7\n",
      "Perfil Cadastrado   =  1\n",
      "Perfil Classificado =  3\n",
      "Severidade =  1\n",
      "/home/ubuntu/data/data/maio_2019/setembro/fas-sep/BT2A411444175.csv\n",
      "################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:766: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    }
   ],
   "source": [
    "processados, erros = [], []\n",
    "sepmm=','\n",
    "sepfas=';'\n",
    "m=loopy2(o_instas, ddirs_fas, dummy_mm, PATH_FAS, PATH_MM, OUT, sepfas, sepmm, gd, tem_perfil_funcionamento, fill_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQ</th>\n",
       "      <th>AVA</th>\n",
       "      <th>AVB</th>\n",
       "      <th>AVC</th>\n",
       "      <th>aab</th>\n",
       "      <th>aac</th>\n",
       "      <th>abc</th>\n",
       "      <th>angcheck</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-06-25 21:30:00</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>120.000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01 05:15:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01 05:30:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01 05:45:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       SEQ    AVA     AVB      AVC   aab    aac   abc  \\\n",
       "data                                                                    \n",
       "2019-06-25 21:30:00  127.0  0.000  60.000  120.000  60.0  120.0  60.0   \n",
       "2019-07-01 05:15:00    NaN  0.001   0.001    0.001   0.0    0.0   0.0   \n",
       "2019-07-01 05:30:00    NaN  0.001   0.001    0.001   0.0    0.0   0.0   \n",
       "2019-07-01 05:45:00    NaN  0.001   0.001    0.001   0.0    0.0   0.0   \n",
       "\n",
       "                     angcheck  \n",
       "data                           \n",
       "2019-06-25 21:30:00      True  \n",
       "2019-07-01 05:15:00      True  \n",
       "2019-07-01 05:30:00      True  \n",
       "2019-07-01 05:45:00      True  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfas.months[2019][7].df[dfas.months[2019][7].df['angcheck'] == True][['SEQ','AVA','AVB','AVC','aab','aac','abc', 'angcheck']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Substituir coluna \"Perfil cadastrado\" por texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l=LISTA_CAD.df.drop(index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tem_perfil_funcionamento:\n",
    "    ra=PERFIS['ramo_de_atividade']\n",
    "    dra=ra.to_dict()\n",
    "    o = pd.read_csv(OUT + 'new.csv')\n",
    "    o['Perfil cadastrado']=o['Nome'].map(dra)\n",
    "    o.to_csv(OUT + 'new2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contar vazios mês (cvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo_faltantes_path = '../../outputs/'\n",
    "fas_nome_arquivo_faltantes = 'fas_vazios_EDP-ES.csv'\n",
    "mm_nome_arquivo_faltantes = 'mm_vazios_EDP-ES.csv'\n",
    "# EDP-ES usa ';', LIGHT = ','\n",
    "sep = ';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "from os import listdir as ld\n",
    "sys.path.append(join('/',*os.getcwd().split('/')[:-1], 'lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import of_proc\n",
    "import mainlib\n",
    "from os import listdir as ld\n",
    "from mainlib import dframe,fasorial\n",
    "from matplotlib import pyplot as plt\n",
    "from of_proc import pdump, pload, dk, daterange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ddirs_fas = {i.split('.')[0] : join(PATH_FAS,i) for i in ld(PATH_FAS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dframe(json_file).data\n",
    "dp = dframe(json_file).dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcols = data['lista2']['columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_months = lcols[7:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_three_months = lcols[8:11]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "date_fields = lcols[3:5] + all_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fields = all_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparser = lambda x: pd.datetime.strptime(x, '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    LISTAF = pd.read_csv(LISTA_CAD_PATH, names = COLS_LISTA, header=0, parse_dates=date_fields, date_parser=dateparser, sep=';',engine='python')\n",
    "except:\n",
    "    LISTAF = pd.read_csv(LISTA_CAD_PATH, names = COLS_LISTA, header=0, parse_dates=date_fields, date_parser=dateparser, engine='python')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LISTAF = LISTAF[LISTAF['instalacao'].isin(faltantes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = LISTAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista.set_index('instalacao', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = lista[lista['troca']=='2-Com troca de medidor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = lista['medidor'].groupby(level=0).apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MM =   '/home/ubuntu/data/data/EDP-ES/mm/csv_proc/#pd_mmproc_comp.csv'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dim = pload('../../pickles/dim.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lista=pload('../../pickles/lista_cad.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leituras faltando - Fasorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_month(df, date, tolerance=10):\n",
    "    begin_date = date - pd.Timedelta(days=30)\n",
    "    success = False\n",
    "    count = 0\n",
    "    while not success and count <= tolerance:\n",
    "        try:\n",
    "            dfo = df[begin_date:date]\n",
    "            success=True\n",
    "        except:\n",
    "            begin_date = begin_date + pd.Timedelta(days=1)\n",
    "            count = count+1\n",
    "    else:\n",
    "        print('Mes nao preenchido')\n",
    "        return None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month(df, date, tolerance=10):\n",
    "    begin_date = date - pd.Timedelta(days=30)\n",
    "    success = False\n",
    "    count = 0\n",
    "    while not success and count <= tolerance:\n",
    "        try:\n",
    "            dfo = df[begin_date:date]\n",
    "            success=True\n",
    "            return dfo\n",
    "        except:\n",
    "            begin_date = begin_date + pd.Timedelta(days=1)\n",
    "            count = count+1\n",
    "    else:\n",
    "        print('Mes nao preenchido')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faso = fasorial(json_file)\n",
    "erros = []\n",
    "emp = {}\n",
    "emp_porc = {}\n",
    "faso=fasorial(json_file)\n",
    "print(len(ddirs_fas))\n",
    "months = {}\n",
    "total_rows = 2880\n",
    "for n,(k,v) in enumerate(ddirs_fas.items()):\n",
    "    print(n, end=' ')\n",
    "    if True:\n",
    "        emp[k]={}\n",
    "        if k.isdigit():\n",
    "            l = lista[all_months].loc[int(k)]\n",
    "        else:\n",
    "            l = lista[all_months].loc[k]\n",
    "        if type(l) == pd.Series:\n",
    "            ddays = dict(l)        \n",
    "        elif type(l) == pd.DataFrame:\n",
    "            ddays = dict(l.groupby(level=0).first().iloc[0])\n",
    "        else:\n",
    "            print('Erro desconhecido: ', k, type(ddays), ddays)\n",
    "        dff = pd.read_csv(v,sep=';',date_parser = dp,parse_dates=[5],names=data['fasorial']['columns'],index_col=5)\n",
    "        dff.sort_index(inplace=True)\n",
    "        for k1,v1 in ddays.items():\n",
    "            months[k1] = get_month(dff, v1)\n",
    "        for k1,v1 in months.items():\n",
    "            months_rows = len(v1)\n",
    "            missing_rows = total_rows - months_rows\n",
    "            if missing_rows > 0:\n",
    "                emp[k][k1] = missing_rows\n",
    "            else:\n",
    "                emp[k][k1] = 0\n",
    "            #emp[k][k1]  = v1[0].resample('15min').first().isna().all(axis=1).sum()\n",
    "    else:\n",
    "        pass\n",
    "        #print(e)\n",
    "        #erros.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmi={}\n",
    "for k,v in dim.items():\n",
    "    for i in v:\n",
    "        dmi[int(i)] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dempt=pd.DataFrame(index=emp.keys(), data=emp.values(),columns = last_three_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dempt.to_csv(arquivo_faltantes_path + fas_nome_arquivo_faltantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testes Resultado Fasorial"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "emp['0000000044']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dff = pd.read_csv('/home/ubuntu/data/data/EDP-ES/fas-sep/0000000289.csv',sep=';',date_parser = dp,parse_dates=[5],names=data['fasorial']['columns'],index_col=5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LISTAF[LISTAF.index==289]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "end_date = pd.to_datetime('2019-08-27')\n",
    "begin_date = end_date - pd.Timedelta(days=30)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dff[begin_date:end_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leituras faltando -  Memória de massa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls /home/ubuntu/data/data/EDP-ES/mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MM = PATH_MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.read_csv(MM, sep=';',decimal=',', low_memory=False, index_col = [0], na_values=' ', date_parser = dp, parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.sort_values(by=['DATA/Hora'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = m.applymap(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = m.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.index = pd.Series(mt.index).apply(float).apply(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.insert(loc=0, value = mt.index.map(dmi).values, column = 'instalacao')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mtf = mt.applymap(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mt.index = mt['instalacao'].apply(int)\n",
    "except:\n",
    "    mt.index = mt['instalacao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.drop(columns='instalacao', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt2 = mt.groupby(level=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = mt2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.index=pd.to_datetime(m2.index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mv = m2.resample('H').min().resample('M').().sum().applymap(lambda x: max(0, 24*30 - x)).T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m2.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mv = m2.resample('H').apply(lambda x: np.nan if x.isna().any() else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_month_2(df, date, tolerance=10):\n",
    "    begin_date = date - pd.Timedelta(days=30)\n",
    "    count = 0\n",
    "#     if pd.datetime.date(begin_date.year, begin_date.month, begin_date.day) not in df.index.date:\n",
    "#         df.loc[begin_date] = np.full(df.shape[1], np.nan)\n",
    "    dfo = df[begin_date:date] \n",
    "    return dfo        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_2(df, date, tolerance=10):\n",
    "    begin_date = date - pd.Timedelta(days=30)\n",
    "    success = False\n",
    "    count = 0\n",
    "    while not success and count <= tolerance:\n",
    "        try:\n",
    "            dfo = df[begin_date:date]\n",
    "            success=True\n",
    "            return dfo\n",
    "        except:\n",
    "            begin_date = begin_date + pd.Timedelta(days=1)\n",
    "            count = count+1\n",
    "    else:\n",
    "        print('Mes nao preenchido')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddays = lista.groupby(level=0)[all_months].first().T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = {k:list(v.values()) for k,v in ddays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddays = dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_m = {}\n",
    "for c in m2.columns:\n",
    "    mem_m[c]={}\n",
    "    for d in ddays[c]:\n",
    "        mem_m[c][d.month] = get_month_2(m2[c], d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = {}\n",
    "for k,v in mem_m.items():\n",
    "    em[k] = {}\n",
    "    for k1,v1 in v.items():\n",
    "        if v1 is not None:\n",
    "            #em[k][k1] = v1.resample('H').max().notnull().sum()\n",
    "            em[k][k1] = v1.resample('H').min().isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for k,v in em.items():\n",
    "    df = pd.concat( (df, pd.DataFrame(index=v.keys(), data=v.values(), columns=[k]).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[all_months]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.name = 'mês'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.applymap(lambda x: max(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2t = df2.T.applymap(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2t.to_csv(arquivo_faltantes_path + mm_nome_arquivo_faltantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testes Resultado Memória de Massa"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m = pd.read_csv('/home/ubuntu/data/data/EDP-ES/mm/csv_proc/#pd_mmproc_comp.csv', sep=';',decimal=',', low_memory=False, index_col = [0], na_values=' ', date_parser = dp, parse_dates=[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dim[3125]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LISTAF[LISTAF.index == 3125]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "end_date = pd.to_datetime('2019-08-26')\n",
    "begin_date = end_date - pd.Timedelta(days=30)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m[begin_date:end_date]['14566517'].resample('H').min().isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
