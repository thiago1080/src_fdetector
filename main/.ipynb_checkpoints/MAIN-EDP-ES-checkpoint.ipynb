{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "sys.path.append(join(Path(os.getcwd()).parent, 'lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.1.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import collections\n",
    "from importlib import reload\n",
    "from  mainlib import fasorial, lista, sdict, mm, mm_sep\n",
    "from unidecode import unidecode\n",
    "from of_proc import *\n",
    "from os.path import join\n",
    "from os import listdir as ld\n",
    "from unidecode import unidecode\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_info(path):\n",
    "    global PATH, OUTPUT_PATH, PROCESSED_PATH, PERFIS_PATH, HORARIOS_PATH, PATH_MM, PATH_FAS, \\\n",
    "    DEG, MOR, PER, FAS, OUTPUT_HEADER,FAS_CONCAT, MM_CONCAT, PICKLES_CONCAT, OUT_INDEX, COLS_LISTA, \\\n",
    "    OUT, DATA_DIR, LISTA_CAD_PATH, PROCESSADOS_PICKLE, ERROS_PICKLE, LOGS_DIR\n",
    "    with open(path) as f:\n",
    "        info = json.load(f)\n",
    "        PATH_FAS = info['paths']['fasoriais2']['input']\n",
    "        PERFIS_PATH = info['paths']['perfis']['perfis']\n",
    "        HORARIOS_PATH = info['paths']['perfis']['horarios']\n",
    "        PATH_MM = info['paths']['mm']['input']\n",
    "        DEG = info['paths']['nn_models']['degrau']\n",
    "        MOR = info['paths']['nn_models']['morro']\n",
    "        PER = info['paths']['nn_models']['perfil']\n",
    "        FAS = info['paths']['nn_models']['fasorial']\n",
    "        COLS_LISTA = info['lista2']['columns']  \n",
    "        OUTPUT_HEADER = info['output']['header']\n",
    "        OUT_INDEX = info['output']['header']\n",
    "        OUT = info['paths']['output']\n",
    "        DATA_DIR = info['paths']['base_dir']\n",
    "        LISTA_CAD_PATH = info['paths']['lista']\n",
    "        PROCESSADOS_PICKLE = info['paths']['processados']\n",
    "        ERROS_PICKLE = info['paths']['erros']\n",
    "        LOGS_DIR = info['paths']['logs']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = '../../info/info-EDP-ES.json'\n",
    "gd = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dk(d,k=0): return list(d.values())[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lista2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1ae798bf9248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mread_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2c1aee9176b5>\u001b[0m in \u001b[0;36mread_info\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mOUT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'base_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mLISTA_CAD_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lista2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mPROCESSADOS_PICKLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processados'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mERROS_PICKLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'erros'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lista2'"
     ]
    }
   ],
   "source": [
    "read_info(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_horario_info2(json_file):\n",
    "    with open(json_file) as jinfo:\n",
    "        info = json.load(jinfo)\n",
    "        HORARIOS_PATH = info['paths']['perfis']['horarios']\n",
    "        PERFIS_PATH = info['paths']['perfis']['perfis']\n",
    "        ATIV_PATH = info['paths']['perfis']['mapa']\n",
    "\n",
    "\n",
    "    perfis = pd.read_csv(PERFIS_PATH, sep = ';')\n",
    "    ativs = pd.read_csv(ATIV_PATH, sep = ';')\n",
    "\n",
    "    #perfis.columns = ['id', 'instalacao', 'cpf_cnpj', 'razao social', 'endereco_cadastro', 'ramo_de_atividade','None1', 'None2']\n",
    "    \n",
    "    dates = re.compile(\"[0-9]{2}:[0-9]{2}\")\n",
    "    dateparser = lambda x: pd.datetime.strptime(x, '%H:%M') if type(x) == str  and dates.match(x) else pd.NaT\n",
    "    cols = ['name','inicio_dia_util','fim_dia_util', 'inicio_sabado', 'fim_sabado', 'inicio_domingo', 'fim_domingo', 'inicio_feriado', 'fim_feriado']\n",
    "    horarios = pd.read_csv(HORARIOS_PATH, sep = ';',skipinitialspace=True, skiprows = 2, na_values = np.NaN, names = cols,\n",
    "                          date_parser = dateparser, parse_dates = [1,2,3,4,5,6,7,8])\n",
    "    \n",
    "    \n",
    "    horarios.set_index('name', inplace = True)\n",
    "    horarios.dropna(inplace=True)\n",
    "\n",
    "    perfis.set_index('instalacao', inplace=True)\n",
    "    perfis = (perfis.groupby(level=0).first())\n",
    "\n",
    "    \n",
    "    return horarios, perfis, ativs\n",
    "\n",
    "def get_files_4(nomes, dim,  CAMINHO):\n",
    "    not_found = []\n",
    "    dif = {}\n",
    "    for i in nomes:\n",
    "        if i in dim:\n",
    "            tmp_list = []\n",
    "            search = ab.search_list_2(str(i),os.listdir(CAMINHO))\n",
    "            if search:\n",
    "                dif[i] = search\n",
    "            else:\n",
    "                not_found.append(i)                \n",
    "    return dif, not_found\n",
    "\n",
    "#Criar dicionario de categorias\n",
    "def insta2horario(HORARIOS_PATH):\n",
    "    horarios = pd.read_csv(HORARIOS_PATH, sep=',')\n",
    "    horarios.drop(columns=horarios.columns[0],inplace=True)\n",
    "    horarios.columns = ['nome', 'cat', 'dias_uteis_i',\n",
    "           'dias_uteis_f', 'sabado_i', 'sabado_f', 'domingo_i',\n",
    "           'domingo_f', 'feriado_i', 'feriado_f']\n",
    "    horarios['nome'] = horarios['nome'].apply(unidecode)\n",
    "    dfcat = horarios.iloc[:,:2]\n",
    "    dfcat.set_index('nome',inplace=True)\n",
    "    scat = dfcat['cat']\n",
    "    dcat = scat.to_dict()\n",
    "    horarios.set_index('nome', inplace=True)\n",
    "    func = {k: [pd.Timestamp(v) for v in horarios.loc[k,'dias_uteis_i':'feriado_f']] for k in horarios.index}\n",
    "    return func\n",
    "\n",
    "def per2cluster(func, ATIV):\n",
    "    dfativ = pd.read_csv(ATIV, sep=';', names = ['ramo', 'perfil'])\n",
    "    dfativ=dfativ.applymap(unidecode)\n",
    "    dfativ.set_index('ramo', inplace=True)\n",
    "    a2p = dfativ.loc[:,'perfil'].T.to_dict()\n",
    "    a2p.pop('Ramo de Atividade')\n",
    "    p2a = dict((v,k) for k,v in a2p.items())\n",
    "    perfunc=PERFIS['ramo_de_atividade'].map(a2p).map(func)\n",
    "    return perfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global df_mm_new\n",
    "mmm = mm(json_file)\n",
    "df_mm_new = pd.read_csv(PATH_MM,date_parser = mmm.dateparser, index_col=[0], sep=';')\n",
    "dummy_mm = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar modelos neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleções principais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global LISTA_CAD\n",
    "global PERFIL, DEGRAU, MORRO, NN_FAS\n",
    "global files_fas, files_mm\n",
    "global HORARIOS, PERFIS, ATIVS, DPERFIS, DPERFIS_NORMAL, DPERFIS_TROCA\n",
    "global dim, dmi, df_fas, dfiles_mm, dfiles_fas_troca, dfiles_mm_troca, dfiles_fas_normal, dfiles_mm_normal\n",
    "global o_i_normal, o_i_troca, o_instas, list_i_normal, list_i_troca, list_intas, gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "PERFIL, DEGRAU, MORRO, NN_FAS = load_models(PER, DEG, MOR, FAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTAF = pd.read_csv(LISTA_CAD_PATH, names = COLS_LISTA, header=0, sep=';',engine='python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTAF['instalacao'] = pd.DataFrame(LISTAF['instalacao'].astype(int).apply(lambda x: '{0:0>10}'.format(x)))\n",
    "LISTAF['medidor'] = pd.DataFrame(LISTAF['medidor'].astype(int).apply(lambda x: '{0:0>8}'.format(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "faltantes = [\n",
    "'0000081002',\n",
    "'0000357754',\n",
    "'0000586609',\n",
    "'0001659074',\n",
    "'0009502437',\n",
    "'0160214293',\n",
    "'0160557791',\n",
    "'0160581342',\n",
    "'0160669410'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTAF = LISTAF[LISTAF['instalacao'].isin(faltantes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTA_CAD = get_list(json_file)\n",
    "LISTA_CAD.df =  LISTAF\n",
    "HORARIOS, PERFIS, ATIVS = get_horario_info2(json_file)\n",
    "dim, dmi = dicts(LISTA_CAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estruturas auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_i_normal = collections.OrderedDict(sorted(LISTA_CAD.mes_days(troca=False).items()))\n",
    "o_i_troca = collections.OrderedDict(sorted(LISTA_CAD.mes_days(troca=True).items()))\n",
    "o_instas =  collections.OrderedDict(sorted({**o_i_normal, **o_i_troca}.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'A_base' from '/home/ubuntu/data/code/ref_fdetector/src/lib/A_base.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import A_base\n",
    "reload(A_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import A_base as ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfiles_fas, nf_fas= get_files_4(list(o_instas.keys()), dim,  PATH_FAS)\n",
    "ddirs_fas = {k:join(PATH_FAS,v) for k,v in dfiles_fas.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista , perfis e horários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário de instalacao para horario de funcionamento\n",
    "func =  insta2horario(HORARIOS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "per2cluster() missing 1 required positional argument: 'ATIV'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b49bcfe1179a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dicionário de perfis para clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mperfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper2cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: per2cluster() missing 1 required positional argument: 'ATIV'"
     ]
    }
   ],
   "source": [
    "# Dicionário de perfis para clusters\n",
    "perfunc = per2cluster(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "percats = PERFIS['cat'].fillna(value=0).apply(int).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " l___________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pdump(o_instas, '../../pickles/o_instas.pkl')\n",
    "pdump(func, '../../pickles/func.pkl')\n",
    "pdump(perfunc, '../../pickles/perfunc.pkl')\n",
    "pdump(ddirs_fas, '../../pickles/ddirs_fas.pkl')\n",
    "pdump(dim, '../../pickles/dim.pkl')\n",
    "pdump(dmi, '../../pickles/dmi.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_hora = []\n",
    "err_msg = {}\n",
    "missing = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSADOS_PICKLE,'rb') as p:\n",
    "    processados=pickle.load(p)\n",
    "with open(ERROS_PICKLE,'rb') as p:\n",
    "    erros = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(os.path.join(LOGS_DIR,str(log_count)), 'at+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(string):\n",
    "    print(string)\n",
    "    log_file.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dici2list(d_insta_to_days):\n",
    "    ddays={}\n",
    "    ddays[4],ddays[5],ddays[6],ddays[7] = d_insta_to_days[12], d_insta_to_days[1], d_insta_to_days[2], d_insta_to_days[3], d_insta_to_days[4]\n",
    "    return ddays\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn' . option None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PERFIS.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "padrao = [pd.Timestamp('2019-09-20 00:00:00'),\n",
    " pd.Timestamp('2019-09-20 23:59:00'),\n",
    " pd.Timestamp('2019-09-20 00:00:00'),\n",
    " pd.Timestamp('2019-09-20 23:59:00'),\n",
    " pd.Timestamp('2019-09-20 00:00:00'),\n",
    " pd.Timestamp('2019-09-20 23:59:00'),\n",
    " pd.Timestamp('2019-09-20 00:00:00'),\n",
    " pd.Timestamp('2019-09-20 23:59:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loopy2(d_insta_to_days, d_insta_to_files_fas, d_insta_to_files_mm,PATH_FAS, PATH_MM, output_dir,sepfas, sepmm, gd):\n",
    "    df_out = pd.DataFrame(index=OUT_INDEX)\n",
    "    global difas, dimm\n",
    "    global dfas, dmm,  dimm\n",
    "    difas, dimm = sdict(), sdict()\n",
    "    c = 0\n",
    "    c1 = 0\n",
    "    for instalacao in d_insta_to_days:\n",
    "        c1 = c1 + 1\n",
    "        if (instalacao  not in processados):\n",
    "            log(str(c)+'--'+'######## ' + instalacao + ' #########')\n",
    "            medidor = dim[instalacao][0] if type(dim[instalacao]) is list else dim[instalacao]\n",
    "            try:      \n",
    "                print('------ 0')\n",
    "                #ramo = percats[instalacao] if instalacao in percats else 0\n",
    "                ramo = 0\n",
    "                print('------ 1')\n",
    "                #hours = perfunc[instalacao] if instalacao in perfunc else padrao\n",
    "                hours = padrao\n",
    "                print('------ 2')\n",
    "                ddays = d_insta_to_days[instalacao] if instalacao in d_insta_to_days else default_ddays_2()\n",
    "                print('------ 3')\n",
    "                dfas = fasorial(json_file, name = instalacao, ddays=ddays)     \n",
    "                print('------ 4')\n",
    "                dfas.outputs['perfil'] = ramo\n",
    "                print('------ 5')\n",
    "                dfas.read_fasorial(d_insta_to_files_fas[instalacao], dtype=None, index_col=[5], sep = sepfas)\n",
    "                print('------ 6')\n",
    "                dmm = mm(json_file, name = instalacao, ddays = ddays, ramo=ramo)\n",
    "                print('------ 7')\n",
    "                if dim[instalacao] in df_mm_new.columns:\n",
    "                    dmm.df = pd.DataFrame(df_mm_new[dim[instalacao]])\n",
    "                else:\n",
    "                    dmm.df = pd.DataFrame(df_mm_new[str(int(dim[instalacao]))])                 \n",
    "\n",
    "                print('------ 8')\n",
    "                proc_fas(dfas,ddays, NN_FAS, json_file, hours = dfas.hours, gd=gd)\n",
    "                processa_mm(dmm, ddays, PERFIL, DEGRAU, MORRO, json_file)\n",
    "                print(d_insta_to_files_fas[instalacao])\n",
    "                print('################################################')\n",
    "                #Cria um dicionário de outputs por mês para o fasorial\n",
    "                for y in dfas.months:\n",
    "                    dfas.outputs[y] = {}\n",
    "                    for month  in dfas.months[y]:\n",
    "                        dfas.outputs[y][month] = dfas.months[y][month].outputs                \n",
    "                df_out = cat_out2(df_out, dmm.outputs, dfas.outputs, ddays,OUT_INDEX)\n",
    "\n",
    "                difas[instalacao] = dfas\n",
    "                dimm[instalacao] = dmm                \n",
    "                c = c+1                \n",
    "                #if c1 % 100 ==0 :  \n",
    "                if False:\n",
    "                    #return df_out, dfas, dmm\n",
    "                    log('Salvando csv!')\n",
    "                    df_out.T.to_csv(output_dir + 'e1_' +str(c1-100) + '_to_' + str(c1) +'.csv')                    \n",
    "                    pickle.dump(processados, open(PROCESSADOS_PICKLE,'wb'))\n",
    "                    pickle.dump(erros, open(ERROS_PICKLE,'wb'))                    \n",
    "                    df_out=pd.DataFrame()\n",
    "                processados.append(instalacao)\n",
    "            except Exception as e:\n",
    "                erros.append(instalacao)\n",
    "                err_msg[instalacao] = traceback.format_exc()\n",
    "                print('++++++Erro 2++++++++++ \\n'+ str(traceback.format_exc())+'\\n++++++++++++++++++++')\n",
    "    df_out.T.to_csv(output_dir + 'new.csv')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mainlib import mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0--######## 0000081002 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "------ 6\n",
      "------ 7\n",
      "------ 8\n",
      "{6: Timestamp('2019-06-14 00:00:00'), 7: Timestamp('2019-07-16 00:00:00'), 8: Timestamp('2019-08-16 00:00:00')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: Timestamp('2019-06-14 00:00:00'), 7: Timestamp('2019-07-16 00:00:00'), 8: Timestamp('2019-08-16 00:00:00')}\n",
      "/home/ubuntu/data/data/EDP-ES/fas-sep/0000081002.csv\n",
      "################################################\n",
      "1--######## 0000357754 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "------ 6\n",
      "------ 7\n",
      "------ 8\n",
      "{6: Timestamp('2019-06-04 00:00:00'), 7: Timestamp('2019-07-04 00:00:00'), 8: Timestamp('2019-08-05 00:00:00')}\n",
      "{6: Timestamp('2019-06-04 00:00:00'), 7: Timestamp('2019-07-04 00:00:00'), 8: Timestamp('2019-08-05 00:00:00')}\n",
      "/home/ubuntu/data/data/EDP-ES/fas-sep/0000357754.csv\n",
      "################################################\n",
      "2--######## 0000586609 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "------ 6\n",
      "------ 7\n",
      "------ 8\n",
      "{6: Timestamp('2019-06-10 00:00:00'), 7: Timestamp('2019-07-10 00:00:00'), 8: Timestamp('2019-08-12 00:00:00')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: Timestamp('2019-06-10 00:00:00'), 7: Timestamp('2019-07-10 00:00:00'), 8: Timestamp('2019-08-12 00:00:00')}\n",
      "/home/ubuntu/data/data/EDP-ES/fas-sep/0000586609.csv\n",
      "################################################\n",
      "3--######## 0001659074 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "++++++Erro 2++++++++++ \n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-33-042eab324cca>\", line 27, in loopy2\n",
      "    dfas.read_fasorial(d_insta_to_files_fas[instalacao], dtype=None, index_col=[5], sep = sepfas)\n",
      "KeyError: '0001659074'\n",
      "\n",
      "++++++++++++++++++++\n",
      "3--######## 0009502437 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "------ 6\n",
      "------ 7\n",
      "------ 8\n",
      "{6: Timestamp('2019-06-25 00:00:00'), 7: Timestamp('2019-07-26 00:00:00'), 8: Timestamp('2019-08-23 00:00:00')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: Timestamp('2019-06-25 00:00:00'), 7: Timestamp('2019-07-26 00:00:00'), 8: Timestamp('2019-08-23 00:00:00')}\n",
      "/home/ubuntu/data/data/EDP-ES/fas-sep/0009502437.csv\n",
      "################################################\n",
      "4--######## 0160214293 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "------ 6\n",
      "------ 7\n",
      "------ 8\n",
      "{6: Timestamp('2019-06-24 00:00:00'), 7: Timestamp('2019-07-26 00:00:00'), 8: Timestamp('2019-08-22 00:00:00')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: Timestamp('2019-06-24 00:00:00'), 7: Timestamp('2019-07-26 00:00:00'), 8: Timestamp('2019-08-22 00:00:00')}\n",
      "/home/ubuntu/data/data/EDP-ES/fas-sep/0160214293.csv\n",
      "################################################\n",
      "5--######## 0160557791 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "------ 6\n",
      "------ 7\n",
      "------ 8\n",
      "{6: Timestamp('2019-06-19 00:00:00'), 7: Timestamp('2019-07-19 00:00:00'), 8: Timestamp('2019-08-21 00:00:00')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: Timestamp('2019-06-19 00:00:00'), 7: Timestamp('2019-07-19 00:00:00'), 8: Timestamp('2019-08-21 00:00:00')}\n",
      "/home/ubuntu/data/data/EDP-ES/fas-sep/0160557791.csv\n",
      "################################################\n",
      "6--######## 0160581342 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "------ 6\n",
      "------ 7\n",
      "------ 8\n",
      "{6: Timestamp('2019-06-25 00:00:00'), 7: Timestamp('2019-07-26 00:00:00'), 8: Timestamp('2019-08-23 00:00:00')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: Timestamp('2019-06-25 00:00:00'), 7: Timestamp('2019-07-26 00:00:00'), 8: Timestamp('2019-08-23 00:00:00')}\n",
      "/home/ubuntu/data/data/EDP-ES/fas-sep/0160581342.csv\n",
      "################################################\n",
      "7--######## 0160669410 #########\n",
      "------ 0\n",
      "------ 1\n",
      "------ 2\n",
      "------ 3\n",
      "------ 4\n",
      "------ 5\n",
      "------ 6\n",
      "------ 7\n",
      "------ 8\n",
      "{6: Timestamp('2019-06-13 00:00:00'), 7: Timestamp('2019-07-15 00:00:00'), 8: Timestamp('2019-08-15 00:00:00')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n",
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{6: Timestamp('2019-06-13 00:00:00'), 7: Timestamp('2019-07-15 00:00:00'), 8: Timestamp('2019-08-15 00:00:00')}\n",
      "/home/ubuntu/data/data/EDP-ES/fas-sep/0160669410.csv\n",
      "################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/code/ref_fdetector/src/lib/mainlib.py:680: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[(f1 & f2) | f3]\n"
     ]
    }
   ],
   "source": [
    "processados, erros = [], []\n",
    "sepmm=','\n",
    "sepfas=';'\n",
    "m=loopy2(o_instas, ddirs_fas, dummy_mm, PATH_FAS, PATH_MM, OUT, sepfas, sepmm, gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Substituir coluna \"Perfil cadastrado\" por texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=LISTA_CAD.df.drop(index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra=PERFIS['ramo_de_atividade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dra=ra.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = pd.read_csv('../../outputs/all/EDP-ES/new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o['Perfil cadastrado']=o['Nome'].map(dra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.to_csv('../../outputs/all/EDP-ES/new2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leituras faltando - Fasorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mainlib\n",
    "reload(mainlib)\n",
    "from mainlib import dframe\n",
    "\n",
    "dfr = dframe()\n",
    "data = dfr.data\n",
    "dp = dfr.dateparser\n",
    "\n",
    "ddirs_fas = pload('../pickles/ddirs_fas.pkl')\n",
    "\n",
    "emp = {}\n",
    "for n,(k,v) in enumerate(ddirs_fas.items()):\n",
    "    df = pd.read_csv(v,sep=';',date_parser = dp,parse_dates=[5],names=data['fasorial']['columns'])\n",
    "    emp[k]=df.set_index('data').iloc[:,0].resample('15min').first().isna().sum()\n",
    "    print(n,k,emp[k])\n",
    "\n",
    "demp=pd.DataFrame(index=emp.keys(), data=emp.values(),columns = ['Leituras faltantes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demp.to_csv('../outputs/fas_emp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leituras faltando - Fasorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = pd.read_csv(PATH_MM, parse_dates = [0],date_parser=dp, index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm[mm.index.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
